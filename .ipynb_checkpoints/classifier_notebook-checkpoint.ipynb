{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot the Classifiers\n",
    "## By Kathrine Gibson and Lucy Tibbetts\n",
    "For our final project, we were interested in how different song attributes affect song popularity. To do this, we used [this dataset from kaggle](https://www.kaggle.com/tomigelo/spotify-audio-features). This dataset contained artist name, track id, track name, audio features, and popularity of over 116k songs on Spotify. Popularity was based on the number of plays (as of December 3, 2018) on a scale of 0 to 100. There were thirteen audio features: acousticness, danceability, duration, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time signature, and valence. All of this data was taken from Spotify's Web API. We found that our k-NN classifier was the most accurate at predicting popularity, followed by our decision tree classifier and our Naive Bayes classifier.\n",
    "\n",
    "To determine which audio features to use, we created graphs of each feature and popularity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "\n",
    "\n",
    "#grid of graphs goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the graphs, we decided not to use key, mode, or time signature. \n",
    "\n",
    "This dataset didn't need much cleaning, but we chose to remove the first three columns of the table since many of the track names included commas. Furthermore, these graphs were created using every song in the dataset, but since our classifiers took significantly longer to run, we created a file which only held the first 2,000 songs. To ensure relatively minimal sampling error, we graphed the data from this file as well to make sure the graphs were similar to the graphs made from the entire dataset. However, this data itself is only a sample of all of the songs which Spotify has available.\n",
    "\n",
    "### KNN Classifier\n",
    "\n",
    "We next implemented our own k-NN classifier to predict a song's popularity using the ten interesting audio features. Most of the audio features are already on a 0 to 1 scale, but we had to normalize duration, loudness, and tempo. Popularity was discretized into >=0.25, >=0.50, >=0.75, >=1.00 prior to classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_data = []\n",
    "utils.read_file_to_table(\"small_audio_data.csv\", trimmed_data, [0, 1, 2, 3, 4, 6, 7, 9, 10, 12, 13])\n",
    "\n",
    "duration = utils.get_column(trimmed_data, 2)\n",
    "normalized_duration = utils.normalize(duration)\n",
    "loudness = utils.get_column(trimmed_data, 6)\n",
    "normalized_loudness = utils.normalize(loudness)\n",
    "tempo = utils.get_column(trimmed_data, 8)\n",
    "normalized_tempo = utils.normalize(tempo)\n",
    "\n",
    "for i in range(len(trimmed_data)):\n",
    "    trimmed_data[i][2] = normalized_duration[i]\n",
    "    trimmed_data[i][6] = normalized_loudness[i]\n",
    "    trimmed_data[i][8] = normalized_tempo[i]\n",
    "    trimmed_data[i][-1] = utils.discretize_popularity(trimmed_data[i][-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalizing our data, we created ten stratified cross folds in order to check the accuracy of our k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy kNN: 74.9%\n"
     ]
    }
   ],
   "source": [
    "folds = utils.stratified_cross_folds(trimmed_data, 10)\n",
    "num_correct = 0\n",
    "for i in range(0, 10):\n",
    "    train, test = utils.set_up_train_test(i, folds)\n",
    "    actual_popularities = [x[-1] for x in test]\n",
    "    predicted_popularities = utils.knn_classifier(train, test)\n",
    "    for i in range(len(test)):\n",
    "        if actual_popularities[i] == predicted_popularities[i]:\n",
    "            num_correct += 1\n",
    "accuracy = num_correct / len(trimmed_data)\n",
    "print(\"Accuracy kNN: \" + str(round(accuracy * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the accuracy of our k-NN classifier on the first 2,000 instances in our dataset was about 74.65%. For this, we had k = 8, but after some experimentation, changing k didn't seem to cause and significant change to the accuracy of the classifier.\n",
    "\n",
    "### Ensemble Classifier\n",
    "\n",
    "Now to see if we could increase our accuracy using k-NN, we implemented a k-NN classifier ensemble with five weak learners. Each of these learners used a different different subset of four attributes. Furthermore, each classifier generated a prediction for the same instance by using different training sets. A singular prediction for each instance was then decide upon by using simple majority voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ensemble kNN: 43.1%\n"
     ]
    }
   ],
   "source": [
    "num_correct_ensemble = 0\n",
    "for i in range(10):\n",
    "    train, test = utils.set_up_train_test(i, folds)\n",
    "    actual_popularities = [x[-1] for x in test]\n",
    "    predicted_popularities = []\n",
    "    for instance in test:\n",
    "        predictions = []\n",
    "        for j in range(6):\n",
    "            # each classifier generates a prediction using a different training set\n",
    "            training_subset = train[j:j+4]\n",
    "            prediction = utils.compute_class_knn(instance, training_subset)\n",
    "            predictions.append(prediction)\n",
    "        # use simple majority voting\n",
    "        np_arr = np.array(predictions)\n",
    "        majority_vote = np.bincount(np_arr).argmax()\n",
    "        predicted_popularities.append(majority_vote)\n",
    "    for i in range(len(test)):\n",
    "        if predicted_popularities[i] == actual_popularities[i]:\n",
    "            num_correct_ensemble += 1\n",
    "accuracy_ensemble = num_correct_ensemble / len(trimmed_data)\n",
    "print(\"Accuracy ensemble kNN: \" + str(round(accuracy_ensemble * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, accuracy was actually lower with our ensemble classifier than with our singular classifier by about 30%. This could be due to the lower number of attributes which each classifer in the ensemble used than the singular classifier used.\n",
    "\n",
    "### Scikit learn\n",
    "\n",
    "We then compared our accuracy results to the accuracy generated using scikit-learn kNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn accuracy (kNN): 74.4%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  import sys\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(trimmed_data)\n",
    "X = np.array(df.ix[:, 0:9])  # features\n",
    "y = np.array(df.ix[:, 10])  # class label (popularity)\n",
    "\n",
    "# split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train, y_train)\n",
    "prediction = knn.predict(X_test)\n",
    "print(\"Scikit-learn accuracy (kNN): \" + str(round(accuracy_score(y_test, prediction) * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of using scikit-learn kNN was actually fairly similar to the accuracy which we got from our own singular k-NN classifier. \n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "With k-NN don, we moved onto an implementation of a TDIDT decision tree classifier. Popularity was already discretized, all attributes were already normalized, and our fingers were ready to pitter patter, and thus we began with some declarations of variables which the classifier would need:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " col_names = [\"acousticness\", \"danceability\", \"duration\", \"energy\",\n",
    "             \"instrumentalness\", \"liveness\", \"loudness\", \"speechiness\", \"tempo\", \"valence\", \"popularity\"]\n",
    "\n",
    "# all possible values for each attribute\n",
    "att_domains = {0: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               1: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               2: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               3: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               4: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               5: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               6: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               7: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               8: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               9: [\">=0.25\", \">=0.50\", \">=0.75\", \">=1.0\"],\n",
    "               10: [\">=25\", \">=50\", \">=75\", \">=100\"]}\n",
    "\n",
    "class_index = len(col_names) - 1\n",
    "\n",
    "# att_indexes is a list of attributes to use for building the tree\n",
    "att_indexes = list(range(len(col_names) - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's looking beautiful, so now let's implement our classifer by using the ten stratified cross folds previously created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Decision Tree: 52.2%\n"
     ]
    }
   ],
   "source": [
    "import tree_utils\n",
    "num_correct = 0\n",
    "for i in range(0, 10):  \n",
    "    train, test = utils.set_up_train_test(i, folds)\n",
    "    actual_popularities = [x[-1] for x in test]\n",
    "    att_indexes = list(range(len(col_names) - 1))\n",
    "    predicted_popularities = tree_utils.tree_classifier(\n",
    "        train, test, att_indexes, att_domains, class_index, col_names)\n",
    "    for i in range(len(test)):\n",
    "        if actual_popularities[i] == predicted_popularities[i]:\n",
    "            num_correct += 1\n",
    "accuracy = num_correct / len(trimmed_data)\n",
    "print(\"Accuracy Decision Tree: \" + str(round(accuracy * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderul, our accuracy was no where near as high as the accuracy of our singular k-NN classifier, but it was higher than the accuracy of our ensemble classifier. This is about the result we expected since decision trees are better suited for categorical attributes.\n",
    "\n",
    "### Naive Bayes\n",
    "\n",
    "Now it was time for our old friend, Naive Bayes. To test the accuracy of our Naive Bayes classifier, we used the same procedure as for our k-NN and decision tree classifiers; we found the total number of correct guesses from ten stratified cross folds and divided by the total number of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Naive Bayes: 43.1%\n"
     ]
    }
   ],
   "source": [
    "num_correct_bayes = 0\n",
    "for i in range(0, 10):\n",
    "    train, test = utils.set_up_train_test(i, folds)\n",
    "    priors = utils.compute_probabilities(train)\n",
    "    actual_popularities_bayes = [x[-1] for x in test]\n",
    "    predicted_popularities_bayes = []\n",
    "    for instance in test:\n",
    "        predicted_popularity_bayes = utils.naive_bayes_classifier(\n",
    "            priors, instance, train)\n",
    "        predicted_popularities_bayes.append(predicted_popularity_bayes)\n",
    "    for i in range(len(test)):\n",
    "        if actual_popularities_bayes[i] == predicted_popularities_bayes[i]:\n",
    "            num_correct_bayes += 1\n",
    "accuracy_bayes = num_correct_bayes / len(trimmed_data)\n",
    "print(\"Accuracy Naive Bayes: \" + str(round(accuracy_bayes * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the accuracy of our Naive Bayes was 43.1%, the same as our ensemble classifier. We predicted that this classifier would result in the lowest accuracy and lucky us, we were right! This was due to the conditional independence assumption, i.e. we expected that the audio features are not wholly independent when predicting popularity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
